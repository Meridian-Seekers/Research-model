{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model traind using coirdinates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels\n",
    "labels = df['class']\n",
    "features = df.drop(columns=['class'])\n",
    "\n",
    "# Ensure all features are numeric\n",
    "features = features.apply(pd.to_numeric, errors='coerce')\n",
    "features = features.fillna(0)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "features_array = features.to_numpy(dtype=np.float32)\n",
    "\n",
    "# **Flatten the features array** to ensure compatibility with the model's expected input shape\n",
    "# Each row should be a vector of length 99 if you have 33 keypoints with x, y, z coordinates\n",
    "features_array = features_array.reshape(features_array.shape[0], -1)  # Shape becomes (samples, 99)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(features_array, labels_categorical, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define a simple classifier model on top of the flattened features\n",
    "def create_classifier_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        Input(shape=(input_shape,)),  # Ensure the input shape is (99,)\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "k = 4  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_full)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    X_train, X_val = X_train_full[train_index], X_train_full[val_index]\n",
    "    y_train, y_val = y_train_full[train_index], y_train_full[val_index]\n",
    "    \n",
    "    # Create and train the classifier model\n",
    "    model = create_classifier_model(X_train.shape[1], labels_categorical.shape[1])\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=5, validation_data=(X_val, y_val), callbacks=[early_stopping], shuffle=True)\n",
    "    \n",
    "    # Evaluate the model on training set\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Evaluate the model on validation set\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Print metrics for the current fold\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Calculate mean and standard deviation for training, validation, and test metrics\n",
    "mean_train_loss = np.mean(train_losses)\n",
    "std_train_loss = np.std(train_losses)\n",
    "mean_train_acc = np.mean(train_accuracies)\n",
    "std_train_acc = np.std(train_accuracies)\n",
    "\n",
    "mean_val_loss = np.mean(val_losses)\n",
    "std_val_loss = np.std(val_losses)\n",
    "mean_val_acc = np.mean(val_accuracies)\n",
    "std_val_acc = np.std(val_accuracies)\n",
    "\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "std_test_loss = np.std(test_losses)\n",
    "mean_test_acc = np.mean(test_accuracies)\n",
    "std_test_acc = np.std(test_accuracies)\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Metrics:\")\n",
    "print(f\"Training Loss: {mean_train_loss:.4f} ± {std_train_loss:.4f}\")\n",
    "print(f\"Training Accuracy: {mean_train_acc:.4f} ± {std_train_acc:.4f}\")\n",
    "print(f\"Validation Loss: {mean_val_loss:.4f} ± {std_val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {mean_val_acc:.4f} ± {std_val_acc:.4f}\")\n",
    "print(f\"Test Loss: {mean_test_loss:.4f} ± {std_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {mean_test_acc:.4f} ± {std_test_acc:.4f}\")\n",
    "\n",
    "# Save the last trained model\n",
    "model.save(\"cnn_model_new3.keras\")\n",
    "\n",
    "# Plot training and validation accuracy for the last fold\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training and validation loss for the last fold\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Check class distribution in the test set\n",
    "test_class_distribution = np.argmax(y_test, axis=1)\n",
    "print(\"Test set class distribution:\", np.bincount(test_class_distribution))\n",
    "\n",
    "# Check class distribution in the predictions\n",
    "predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "print(\"Predicted class distribution:\", np.bincount(predictions))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_class_distribution, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model train ed using image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Paths to the image dataset directories\n",
    "train_dir = \"Dataset/train\"\n",
    "val_dir = \"Dataset/test\"\n",
    "\n",
    "# Define image size and batch size\n",
    "img_height, img_width = 224,224\n",
    "batch_size = 16\n",
    "\n",
    "# Set up ImageDataGenerator with data augmentation for train and validation data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True  # Ensure data is shuffled\n",
    ")\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # No shuffle for validation data to maintain consistency\n",
    ")\n",
    "\n",
    "# Set up the modified CNN model with dropout and batch normalization\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    with tf.device('/CPU:0'):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    \n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            \n",
    "\n",
    "            tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "           \n",
    "            \n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = train_data.num_classes\n",
    "model = create_cnn_model((img_height, img_width, 3), num_classes)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-3 * 0.9 ** epoch)\n",
    "# Train the model with the generator\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=25,  # Increased epochs to allow more training with early stopping\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_acc = model.evaluate(val_data)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Predictions and Confusion Matrix\n",
    "val_data.reset()  # Reset generator for reproducible results\n",
    "predictions = np.argmax(model.predict(val_data), axis=-1)\n",
    "true_labels = val_data.classes\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=val_data.class_indices.keys(), yticklabels=val_data.class_indices.keys())\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"cnn_model_augmented_dropout2.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
